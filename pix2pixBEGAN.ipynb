{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zachary.preprocess.datasets import AtemporalDataset, TemporalDataset, AtemporalMidiDataset, TemporalMidiDataset\n",
    "from zachary.preprocess.base import Configuration\n",
    "from zachary.preprocess.utils import load_audio_file, spectrum_from_signal\n",
    "from zachary.weight_initializers import initialize_model\n",
    "from zachary.utils import get_torch_device, get_num_trainable_params\n",
    "from zachary.model.autoencoder import Autoencoder\n",
    "from zachary.model.generator import Generator\n",
    "from zachary.model.discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum(spect):\n",
    "    plt.rcParams['figure.figsize'] = (19, 6)\n",
    "\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    ax1.imshow(spect, aspect='auto', interpolation='none', origin='lower')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_to_signal(S, num_iters=15):\n",
    "    # Retrieve phase information\n",
    "    phase = 2 * np.pi * np.random.random_sample(S.shape) - np.pi\n",
    "    signal = None\n",
    "    for idx in range(num_iters):\n",
    "        D = S * np.exp(1j * phase)\n",
    "        signal = librosa.istft(D, hop_length=conf.hop_length, win_length=conf.frame_length)\n",
    "        # don't calculate phase during the last iteration, because it will not be used.\n",
    "        if idx < num_iters - 1:\n",
    "            phase = np.angle(librosa.stft(signal, n_fft=conf.frame_length, hop_length=conf.hop_length))\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "DEVICE = get_torch_device()\n",
    "\n",
    "conf = Configuration(audio_dir='/home/kureta/Music/Beethoven Piano Sonatas Barenboim/small/',\n",
    "                     midi_dir='/home/kureta/Music/midi/beethoven/small/')\n",
    "\n",
    "audio_dataset = AtemporalDataset(conf=conf)\n",
    "audio_loader = DataLoader(audio_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(513, 32, 3)\n",
    "print(get_num_trainable_params(autoencoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.train()\n",
    "autoencoder.to(DEVICE)\n",
    "\n",
    "for i in tnrange(50, desc='Epochs'):\n",
    "    step = 0\n",
    "    progress = tqdm_notebook(audio_loader, total=len(audio_dataset)//BATCH_SIZE)\n",
    "    for x in progress:\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        ae_optimizer.zero_grad()\n",
    "        x_hat = autoencoder(x)\n",
    "        loss = ae_criterion(x_hat, x)\n",
    "        loss.backward()\n",
    "        ae_optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            progress.set_description(f'Loss: {loss:.2e}')\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = audio_dataset[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(example.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    x_hat = autoencoder(example.to(DEVICE)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(x_hat.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_hat = stft_to_signal(x_hat.numpy().T * audio_dataset.maxima.numpy(), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=audio_hat, rate=conf.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_a_b = Generator(513, 128, 64, 4)\n",
    "gen_b_a = Generator(128, 513, 64, 4)\n",
    "disc_a = Discriminator(513)\n",
    "disc_b = Discriminator(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen = torch.optim.Adam(chain(gen_a_b.parameters(), gen_b_a.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_disc_a = torch.optim.Adam(disc_a.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_disc_b = torch.optim.Adam(disc_b.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_a_b.train()\n",
    "gen_b_a.train()\n",
    "disc_a.train()\n",
    "disc_b.train()\n",
    "gen_a_b.to(DEVICE)\n",
    "gen_b_a.to(DEVICE)\n",
    "disc_a.to(DEVICE)\n",
    "disc_b.to(DEVICE)\n",
    "\n",
    "\n",
    "sizes = [8, 16, 32, 64]\n",
    "for i, size in enumerate(sizes):\n",
    "    dataset_a.example_length = size\n",
    "    dataset_b.example_length = size\n",
    "    dataset_a.example_hop_length = size // 2\n",
    "    dataset_b.example_hop_length = size // 2\n",
    "    step = 0\n",
    "    \n",
    "    for example_a, example_b in zip(data_loader_a, data_loader_b):\n",
    "        example_a = example_a.to(DEVICE)\n",
    "        example_b = example_b.to(DEVICE)\n",
    "        \n",
    "        optimizer_gen.zero_grad()\n",
    "        \n",
    "        # Identity loss\n",
    "#         a_id = gen_b_a(example_a)\n",
    "#         b_id = gen_a_b(example_b)\n",
    "#         loss_id = criterion_identity(example_a, a_id) + criterion_identity(example_b, b_id)\n",
    "        \n",
    "        # GAN loss\n",
    "        a_hat = gen_b_a(example_b)\n",
    "        b_hat = gen_a_b(example_a)\n",
    "#         with torch.no_grad():\n",
    "        is_fake_a = disc_a(a_hat)\n",
    "        is_fake_b = disc_b(b_hat)\n",
    "        loss_GAN = criterion_GAN(is_fake_a, torch.ones_like(is_fake_a)) + criterion_GAN(is_fake_b, torch.ones_like(is_fake_b))\n",
    "        \n",
    "        # Cycle loss\n",
    "        cycled_a = gen_b_a(b_hat)\n",
    "        cycled_b = gen_a_b(a_hat)\n",
    "        loss_cycle = criterion_cycle(cycled_a, example_a) + criterion_cycle(cycled_b, example_b)\n",
    "        \n",
    "        # Total generator loss\n",
    "        # loss_gen = 0.5 * loss_id + 10.0 * loss_cycle + loss_GAN\n",
    "        loss_gen = 10.0 * loss_cycle + loss_GAN\n",
    "        loss_gen.backward(retain_graph=True)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            # optimize generators\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            optimizer_disc_a.zero_grad()\n",
    "\n",
    "            # Disc A loss\n",
    "            is_real_a = disc_a(example_a)\n",
    "    #         is_fake_a = disc_a(a_hat.unsqueeze(1))\n",
    "            loss_disc_a = criterion_GAN(is_real_a, torch.ones_like(is_real_a)) + criterion_GAN(is_fake_a, torch.zeros_like(is_fake_a))\n",
    "            loss_disc_a.backward()\n",
    "\n",
    "            # Optimize Discriminator A\n",
    "            optimizer_disc_a.step()\n",
    "\n",
    "            optimizer_disc_b.zero_grad()\n",
    "\n",
    "            # Disc B loss\n",
    "            is_real_b = disc_b(example_b)\n",
    "    #         is_fake_b = disc_b(b_hat.unsqueeze(1))\n",
    "            loss_disc_b = criterion_GAN(is_real_b, torch.ones_like(is_real_b)) + criterion_GAN(is_fake_b, torch.zeros_like(is_fake_b))\n",
    "            loss_disc_b.backward()\n",
    "\n",
    "            # Optimize Discriminator B\n",
    "            optimizer_disc_b.step()\n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            print(f'({size}) iteration: {step}/{dataset_b.midi.shape[0]}, generator_loss: {loss_gen:.4e}, cycle_loss: {loss_cycle:.4e}, '\n",
    "                  f'gan_loss: {loss_GAN:.4e}, disc_loss: {loss_disc_a + loss_disc_b:.4e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a = '/home/kureta/Music/Beethoven Piano Sonatas Barenboim/split-track01.ape'\n",
    "path_b = '/home/kureta/Music/midi/beethoven/small/appass_1.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zachary.preprocess.base import load_midi_file, trim_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_a = load_audio_file(path_a, conf)\n",
    "midi_b = load_midi_file(path_b, conf)\n",
    "midi_b = trim_zeros(midi_b).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_a, rate=conf.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(midi_b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_a = torch.from_numpy(librosa.amplitude_to_db(spectrum_from_signal(audio_a, conf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(spectrum_a.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_a_b.eval()\n",
    "with torch.no_grad():\n",
    "    midi_b_hat = gen_a_b(spectrum_a.transpose(0, 1).unsqueeze(0).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(midi_b_hat.squeeze(0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_b_hat.min(), midi_b_hat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_b_a.eval()\n",
    "with torch.no_grad():\n",
    "    spectrum_a_hat = gen_b_a(torch.from_numpy(midi_b).transpose(0, 1).unsqueeze(0).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(spectrum_a_hat.squeeze(0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spectrum_a_hat.squeeze(0)[:, :1000].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_a_hat = stft_to_signal(librosa.db_to_amplitude(s.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_a_hat, rate=conf.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zachary-venv",
   "language": "python",
   "name": "zachary-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
