{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zachary.datasets import AudioDataset, SpectrumDataset\n",
    "from zachary.utils import get_torch_device, get_num_trainable_params\n",
    "from zachary.weight_initializers import initialize_model\n",
    "from zachary.plotting import plot_mag_phase\n",
    "from zachary.modules import SeparableConv1d, SeparableConvTranspose1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "DEVICE = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectrumDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.example_length = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].shape, dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum(y):\n",
    "    plt.rcParams['figure.figsize'] = (18, 4)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1)\n",
    "    ax1.imshow(y.numpy(), aspect='auto', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(x):\n",
    "    plt.rcParams['figure.figsize'] = (18, 4)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1)\n",
    "    ax1.plot(x[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(dataset[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum(dataset[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[8, 16, 32, 64, 128], separable=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        if separable:\n",
    "            convolution = SeparableConv1d\n",
    "        else:\n",
    "            convolution = nn.Conv1d\n",
    "        \n",
    "        entry_layers = 8\n",
    "        entry_channels = 8\n",
    "        kernel_channels = [entry_channels * entry_layers] + channels\n",
    "        kernel_sizes = [4, 4, 4, 4, 4]\n",
    "        \n",
    "        self.e1 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**0, padding=2**0)\n",
    "        self.e2 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**1, padding=2**1)\n",
    "        self.e3 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**2, padding=2**2)\n",
    "        self.e4 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**3, padding=2**3)\n",
    "        self.e5 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**4, padding=2**4)\n",
    "        self.e6 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**5, padding=2**5)\n",
    "        self.e7 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**6, padding=2**6)\n",
    "        self.e8 = convolution(in_channels=1, out_channels=entry_channels, kernel_size=3, dilation=2**7, padding=2**7)\n",
    "        \n",
    "        \n",
    "        self.c1 = convolution(in_channels=kernel_channels[0], out_channels=kernel_channels[1], kernel_size=kernel_sizes[0], stride=kernel_sizes[0])\n",
    "        self.c2 = convolution(in_channels=kernel_channels[1], out_channels=kernel_channels[2], kernel_size=kernel_sizes[1], stride=kernel_sizes[1])\n",
    "        self.c3 = convolution(in_channels=kernel_channels[2], out_channels=kernel_channels[3], kernel_size=kernel_sizes[2], stride=kernel_sizes[2])\n",
    "        self.c4 = convolution(in_channels=kernel_channels[3], out_channels=kernel_channels[4], kernel_size=kernel_sizes[3], stride=kernel_sizes[3])\n",
    "        self.c5 = convolution(in_channels=kernel_channels[4], out_channels=kernel_channels[5], kernel_size=kernel_sizes[4], stride=kernel_sizes[4] // 2)\n",
    "        \n",
    "        self.entry = [self.e1, self.e2, self.e3, self.e4, self.e5, self.e6, self.e7, self.e8]\n",
    "        self.convolutions = [self.c1, self.c2, self.c3, self.c4, self.c5]\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = []\n",
    "        for layer in self.entry:\n",
    "            xs.append(F.relu(layer(x)))\n",
    "        xs = torch.cat(xs, dim=1)\n",
    "        \n",
    "        for layer in self.convolutions[:-1]:\n",
    "            xs = F.relu(layer(xs))\n",
    "        xs = self.convolutions[-1](xs)\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 128, 64, 32, 16], separable=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        if separable:\n",
    "            deconvolution = SeparableConvTranspose1d\n",
    "        else:\n",
    "            deconvolution = nn.ConvTranspose1d\n",
    "        \n",
    "        sizes = [8, 4, 4, 4, 2]\n",
    "        \n",
    "        self.c1 = deconvolution(in_channels=channels[0], out_channels=channels[1], kernel_size=sizes[0], stride=sizes[0]//8)\n",
    "        self.c2 = deconvolution(in_channels=channels[1], out_channels=channels[2], kernel_size=sizes[1], stride=sizes[1])\n",
    "        self.c3 = deconvolution(in_channels=channels[2], out_channels=channels[3], kernel_size=sizes[2], stride=sizes[2])\n",
    "        self.c4 = deconvolution(in_channels=channels[3], out_channels=channels[4], kernel_size=sizes[3], stride=sizes[3])\n",
    "        self.c5 = deconvolution(in_channels=channels[4], out_channels=1, kernel_size=sizes[4], stride=sizes[4])\n",
    "        \n",
    "        self.convolutions = [self.c1, self.c2, self.c3, self.c4, self.c5]\n",
    "\n",
    "    def forward(self, z):\n",
    "        for layer in self.convolutions[:-1]:\n",
    "            z = F.relu(layer(z))\n",
    "        return self.convolutions[-1](z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, channels=[8, 16, 32, 64, 128], separable=False):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(channels, separable)\n",
    "        channels.reverse()\n",
    "        self.decoder = Decoder(channels, separable)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, channels=[8, 16, 32, 64, 128], separable=False):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(channels, separable)\n",
    "        channels.reverse()\n",
    "        self.decoder = Decoder(channels, separable)\n",
    "        ch = self.encoder.c5.out_channels\n",
    "        self.mu_layer = nn.Conv1d(ch, ch, 3, 1, 1)\n",
    "        self.logvar_layer = nn.Conv1d(ch, ch, 3, 1, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.encoder(x))\n",
    "        return self.mu_layer(h1), self.logvar_layer(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder([513, 256, 128, 64, 32])\n",
    "initialize_model(model)\n",
    "print(get_num_trainable_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_loss = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(result, x):\n",
    "    x_hat, mu, logvar = result\n",
    "    mse = F.mse_loss(x_hat, x)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return mse + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diger_loss(signal_hat, signal):\n",
    "    mse = F.mse_loss(signal_hat, signal)\n",
    "    cosine_loss = 1 - F.cosine_similarity(signal_hat.squeeze(1), signal.squeeze(1)).mean()\n",
    "    return cosine_loss + mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, AutoEncoder):\n",
    "    loss_fn = ae_loss\n",
    "elif isinstance(model, VAE):\n",
    "    loss_fn = vae_loss\n",
    "else:\n",
    "    print('UNKNOWN MODEL')\n",
    "    loss_fn = diger_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase `example_length` during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_length = 11\n",
    "dataset.example_length = example_length\n",
    "data_loader = DataLoader(dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.train()\n",
    "for i in range(100):\n",
    "    batch = 1\n",
    "    with tqdm_notebook(total=dataset.examples.shape[0]) as pbar:\n",
    "        for example, target in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(example.to(DEVICE)), target.to(DEVICE))\n",
    "\n",
    "            pbar.set_description(f'Epoch: {i + 1} - loss: {loss.data.cpu().numpy():.4f}')\n",
    "            pbar.update(example.shape[0])\n",
    "\n",
    "            batch += 1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = dataset.audio[:30*44100].unsqueeze(0).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.spectrum[:,:30*100].unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_hat = model(sample)\n",
    "\n",
    "if isinstance(model, VAE):\n",
    "    sample_hat, _, _ = sample_hat\n",
    "\n",
    "sample_hat = sample_hat.squeeze(0).cpu()\n",
    "sample_hat_np = sample_hat.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(sample_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sample_hat_np, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(dataset.audio[:sample_hat.shape[1]].unsqueeze(0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(dataset.audio[:sample_hat.shape[1]].cpu().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(sample)\n",
    "\n",
    "if isinstance(model, VAE):\n",
    "    z, _ = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 4)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.imshow(z[0].cpu().numpy(), aspect='auto')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bspline(cv, n=100, degree=3, periodic=False):\n",
    "    # If periodic, extend the point array by count+degree+1\n",
    "    if degree < 1:\n",
    "        raise ValueError('degree cannot be less then 1!')\n",
    "    count = len(cv)\n",
    "\n",
    "    if periodic:\n",
    "        factor, fraction = divmod(count + degree + 1, count)\n",
    "        cv = np.concatenate((cv,) * factor + (cv[:fraction],))\n",
    "        count = len(cv)\n",
    "\n",
    "    # If opened, prevent degree from exceeding count-1\n",
    "    else:\n",
    "        1if count < degree + 1:\n",
    "            raise ValueError('number of cvs must be higher than degree + 1')\n",
    "\n",
    "    # Calculate knot vector\n",
    "    if periodic:\n",
    "        kv = np.arange(0 - degree, count + degree + degree - 1, dtype='int')\n",
    "    else:\n",
    "        kv = np.array([0] * degree + list(range(count - degree + 1)) + [count - degree] * degree, dtype='int')\n",
    "\n",
    "    # Calculate query range\n",
    "    u = np.linspace(periodic, (count - degree), n)\n",
    "\n",
    "    # Calculate result\n",
    "    arange = np.arange(len(u))\n",
    "    points = np.zeros((len(u), cv.shape[1]))\n",
    "    for i in range(cv.shape[1]):\n",
    "        points[arange, i] = si.splev(u, (kv, cv[:, i], degree))\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def sample_z(z_dims, mean, std, num_cv, resolution, degree, is_periodic):\n",
    "    # Generates splines of random lengths in z_dims dimensions\n",
    "    # num_cv = np.random.randint(64, 128)\n",
    "    cv = np.random.normal(mean, std, (num_cv, z_dims))\n",
    "    num_points = num_cv * resolution\n",
    "    spline = bspline(cv, num_points, degree, is_periodic)\n",
    "    return spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = sample_z(model.encoder.c5.out_channels, 0., 2., 100, 10, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_t = torch.from_numpy(zs.astype('float32').T).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = model.decode(zs_t)\n",
    "\n",
    "y_hat = y.squeeze(0).cpu()\n",
    "y_hat_np = y_hat.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(y_hat_np, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zachary-ipyk",
   "language": "python",
   "name": "zachary-ipyk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
