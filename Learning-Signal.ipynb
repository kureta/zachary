{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zachary.datasets import AtemporalDataset\n",
    "from zachary.constants import Configuration\n",
    "from zachary.sampling import sample_z\n",
    "from zachary.utils import get_torch_device, get_num_trainable_params\n",
    "from zachary.weight_initializers import initialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "DEVICE = get_torch_device()\n",
    "conf = Configuration(audio_dir='/home/kureta/Music/V24', hop_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AtemporalDataset(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(data, start, stop, idx):\n",
    "    if len(data[0][idx].shape) == 0:\n",
    "        result = torch.zeros((stop - start, ))\n",
    "    else:\n",
    "        result = torch.zeros((stop - start, data[0][idx].shape[0]))\n",
    "    for i in range(start, stop):\n",
    "        result[i - start] = data[i][idx]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 4)\n",
    "specgram = get_idx(dataset, conf.time_to_frames(30), conf.time_to_frames(40), 0)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.imshow(specgram.t(), aspect='auto', origin='lower')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_to_signal(S, num_iters=15):\n",
    "    S_T = S.T\n",
    "\n",
    "    # Retrieve phase information\n",
    "    phase = 2 * np.pi * np.random.random_sample(S_T.shape) - np.pi\n",
    "    signal = None\n",
    "    for idx in range(num_iters):\n",
    "        D = S_T * np.exp(1j * phase)\n",
    "        signal = librosa.istft(D, hop_length=conf.hop_length, win_length=conf.frame_length)\n",
    "        # don't calculate phase during the last iteration, because it will not be used.\n",
    "        if idx < num_iters - 1:\n",
    "            phase = np.angle(librosa.stft(signal, n_fft=conf.frame_length, hop_length=conf.hop_length))\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = stft_to_signal((specgram * dataset.maxima).numpy(), num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 4)\n",
    "t = np.linspace(0, len(sig) / conf.sample_rate, len(sig))\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.plot(t, sig)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sig, rate=conf.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c1 = nn.Linear(513, 256)\n",
    "        self.c2 = nn.Linear(256, 128)\n",
    "        self.c3 = nn.Linear(128, 64)\n",
    "        self.c4 = nn.Linear(64, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.c1(x))\n",
    "        z = F.relu(self.c2(z))\n",
    "        z = F.relu(self.c3(z))\n",
    "        z = self.c4(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c2 = nn.Linear(8+3, 64)\n",
    "        self.c3 = nn.Linear(64, 128)\n",
    "        self.c4 = nn.Linear(128, 256)\n",
    "        self.c5 = nn.Linear(256, 513)\n",
    "\n",
    "    def forward(self, z, *labels):\n",
    "        x = F.relu(self.c2(torch.cat([z, *labels], 1)))\n",
    "        x = F.relu(self.c3(x))\n",
    "        x = F.relu(self.c4(x))\n",
    "        x = torch.sigmoid(self.c5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = LabelDecoder()\n",
    "    \n",
    "    def forward(self, x, *labels):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z, *labels)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c2 = nn.Linear(8, 64)\n",
    "        self.c3 = nn.Linear(64, 128)\n",
    "        self.c4 = nn.Linear(128, 256)\n",
    "        self.c5 = nn.Linear(256, 513)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.c2(z))\n",
    "        x = F.relu(self.c3(x))\n",
    "        x = F.relu(self.c4(x))\n",
    "        x = torch.sigmoid(self.c5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LabelAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    pin_memory=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(DEVICE)\n",
    "for i in range(5):\n",
    "    batch = 1\n",
    "    with tqdm_notebook(total=len(dataset)) as pbar:\n",
    "        for spectrum, pitch, confidence, loudness in data_loader:\n",
    "            spectrum = spectrum.to(DEVICE)\n",
    "            pitch = pitch.to(DEVICE).unsqueeze(1)\n",
    "            confidence = confidence.to(DEVICE).unsqueeze(1)\n",
    "            loudness = loudness.to(DEVICE).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            spectrum_hat = model(spectrum, pitch, confidence, loudness)\n",
    "            loss = loss_function(spectrum_hat, spectrum)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_description(\n",
    "                f'Epoch: {i + 1} - loss: {loss.data.cpu().numpy():.2E}')\n",
    "            pbar.update(spectrum.shape[0])\n",
    "\n",
    "            batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_idx(dataset, conf.time_to_frames(10), conf.time_to_frames(20), 0).to(DEVICE)\n",
    "f0s =  get_idx(dataset, conf.time_to_frames(10), conf.time_to_frames(20), 1).to(DEVICE).unsqueeze(1)\n",
    "confidences =  get_idx(dataset, conf.time_to_frames(10), conf.time_to_frames(20), 2).to(DEVICE).unsqueeze(1)\n",
    "loudnesses =  get_idx(dataset, conf.time_to_frames(10), conf.time_to_frames(20), 3).to(DEVICE).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape, f0s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_hat = model(sample, f0s, confidences, loudnesses)\n",
    "\n",
    "sample_hat = sample_hat.cpu() * dataset.maxima\n",
    "sample_hat_np = sample_hat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_hat = stft_to_signal(sample_hat_np, num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 4)\n",
    "t = np.linspace(0, len(signal_hat) / 44100, len(signal_hat))\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.plot(t, signal_hat)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(signal_hat, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "num_cv = conf.time_to_frames(10) // resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = sample_z(8, 0., 1., num_cv, resolution, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_t = torch.from_numpy(zs.astype('float32')).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = torch.zeros((zs_t.shape[0], f0s.shape[1]))\n",
    "constant[:] = 0.3\n",
    "constant = constant.to(DEVICE)\n",
    "\n",
    "constant1 = torch.zeros((zs_t.shape[0], confidences.shape[1]))\n",
    "constant1[:] = 0.9\n",
    "constant1 = constant1.to(DEVICE)\n",
    "\n",
    "constant2 = torch.zeros((zs_t.shape[0], loudnesses.shape[1]))\n",
    "constant2[:] = 0.5\n",
    "constant2 = constant2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = model.decoder(zs_t, constant, constant1, constant2)\n",
    "\n",
    "y_hat = y.cpu() * dataset.maxima\n",
    "y_hat_np = y_hat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hat = stft_to_signal(y_hat_np, num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 4)\n",
    "t = np.linspace(0, len(s_hat) / 44100, len(s_hat))\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.plot(t, s_hat)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(s_hat, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zachary-ipyk",
   "language": "python",
   "name": "zachary-ipyk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
