{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchbearer\n",
    "from torchbearer.cv_utils import DatasetValidationSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zachary.audio_data import AudioDataset\n",
    "import zachary.transforms as transforms\n",
    "from zachary.utils import get_torch_device, get_num_trainable_params, initialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.1\n",
    "DEVICE = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.example_length = 3\n",
    "splitter = DatasetValidationSplitter(len(dataset), VALIDATION_SPLIT)\n",
    "train_dataset = splitter.get_train_dataset(dataset)\n",
    "val_dataset = splitter.get_val_dataset(dataset)\n",
    "traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valgen = torch.utils.data.DataLoader(val_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (26, 8)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.matshow(dataset[1000][0].numpy(), aspect='auto', interpolation='none', origin='lower')\n",
    "ax1.set_title('Mu-Law encoded frame')\n",
    "\n",
    "ax2.matshow(dataset[1000][1].numpy(), aspect='auto', interpolation='none', origin='lower')\n",
    "ax2.set_title('Mu-Law expanded frame')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layers_weights_init(model):\n",
    "    for m in model.modules():\n",
    "        classname = m.__class__.__name__\n",
    "\n",
    "        if 'Conv' in classname:\n",
    "            try:\n",
    "                size = m.weight.shape[0] * m.weight.shape[2]\n",
    "                m.weight.data.normal_(0.0, size.reciprocal().sqrt())\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        elif 'BatchNorm' in classname:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConv1d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConvTransposed1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConvTransposed1d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose1d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n",
    "                                        stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.ConvTranspose1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        channels = [1026, 512, 128, 32]\n",
    "        \n",
    "        layers = OrderedDict([\n",
    "            ('conv1d_01', SeparableConv1d(channels[0], channels[1], kernel_size=3, padding=1)),\n",
    "            ('relu_01', nn.SELU()),\n",
    "            ('conv1d_02', SeparableConv1d(channels[1], channels[2], kernel_size=2)),\n",
    "            ('relu_02', nn.SELU()),\n",
    "            ('conv1d_03', SeparableConv1d(channels[2], channels[3], kernel_size=2)),\n",
    "            (('sigmoid_01'), nn.Sigmoid()),\n",
    "#             ('relu_03', nn.SELU()),\n",
    "#             ('conv1d_04', nn.Conv1d(channels[3], channels[4], kernel_size=1, bias=False)),\n",
    "        ])\n",
    "        \n",
    "        self.block = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        channels = [32, 128, 512, 1026]\n",
    "        \n",
    "        layers = OrderedDict([\n",
    "            ('conv1d_01', SeparableConvTransposed1d(channels[0], channels[1], kernel_size=2)),\n",
    "            ('relu_01', nn.SELU()),\n",
    "            ('conv1d_02', SeparableConvTransposed1d(channels[1], channels[2], kernel_size=2)),\n",
    "            ('relu_02', nn.SELU()),\n",
    "            ('conv1d_03', SeparableConvTransposed1d(channels[2], channels[3], kernel_size=3, padding=1)),\n",
    "#             ('relu_03', nn.SELU()),\n",
    "#             ('conv1d_04', nn.ConvTranspose1d(channels[3], channels[4], kernel_size=1, bias=False)),\n",
    "        ])\n",
    "        \n",
    "        self.block = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('encoder', self.encoder),\n",
    "            ('decoder', self.decoder)\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder()\n",
    "initialize_model(ae)\n",
    "ae.to(DEVICE)\n",
    "print(get_num_trainable_params(ae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, ae.parameters()), lr=0.001)\n",
    "trial = torchbearer.Trial(ae, optimizer, F.mse_loss, metrics=['loss']).to(DEVICE)\n",
    "trial.with_generators(train_generator=traingen, val_generator=valgen)\n",
    "trial.run(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.eval()\n",
    "tmp = dataset.example_length\n",
    "dataset.example_length = 1000\n",
    "stft = dataset[0][0].unsqueeze(0).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    stft_hat = ae(stft)\n",
    "dataset.example_length = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_np = stft.cpu().squeeze().numpy()\n",
    "stft_np = stft_np[:513, :] + 1j * stft_np[513:, :]\n",
    "audio_np = librosa.istft(stft_np, hop_length=512, center=False)\n",
    "stft_hat_np = stft_hat.cpu().squeeze().numpy()\n",
    "stft_hat_np = stft_hat_np[:513, :] + 1j * stft_hat_np[513:, :]\n",
    "audio_hat_np = librosa.istft(stft_hat_np, hop_length=512, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (24, 8)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "ax1.plot(audio_np)\n",
    "ax1.set_title('Real frame')\n",
    "\n",
    "ax2.plot(audio_hat_np[1024:-1024])\n",
    "ax2.set_title('Autoencoded frame')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_hat_np[1024:-1024], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_np, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.example_length = 13\n",
    "splitter = DatasetValidationSplitter(len(dataset), VALIDATION_SPLIT)\n",
    "train_dataset = splitter.get_train_dataset(dataset)\n",
    "val_dataset = splitter.get_val_dataset(dataset)\n",
    "traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valgen = torch.utils.data.DataLoader(val_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.BCELoss()\n",
    "# valid = torch.ones(BATCH_SIZE, 32, dataset.example_length-2, device=DEVICE)\n",
    "# fake = torch.zeros(BATCH_SIZE, 32, dataset.example_length-2, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_IMGS = torchbearer.state_key('gen_imgs')\n",
    "DISC_GEN = torchbearer.state_key('disc_gen')\n",
    "DISC_GEN_DET = torchbearer.state_key('disc_gen_det')\n",
    "DISC_REAL = torchbearer.state_key('disc_real')\n",
    "G_LOSS = torchbearer.state_key('g_loss')\n",
    "D_LOSS = torchbearer.state_key('d_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, encoding_length):\n",
    "        super().__init__()\n",
    "        self.encoding_length = encoding_length\n",
    "        self.discriminator = Encoder()\n",
    "        self.generator = Decoder()\n",
    "\n",
    "    def forward(self, real_imgs, state):\n",
    "        # Generator Forward\n",
    "        z = torch.Tensor(np.random.normal(0, 1, (real_imgs.shape[0], 32, self.encoding_length))).to(state[torchbearer.DEVICE])\n",
    "        state[GEN_IMGS] = self.generator(z)\n",
    "        state[DISC_GEN] = self.discriminator(state[GEN_IMGS])\n",
    "        # This clears the function graph built up for the discriminator\n",
    "        self.discriminator.zero_grad()\n",
    "\n",
    "        # Discriminator Forward\n",
    "        state[DISC_GEN_DET] = self.discriminator(state[GEN_IMGS].detach())\n",
    "        state[DISC_REAL] = self.discriminator(real_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torchbearer.callbacks.add_to_loss\n",
    "def loss_callback(state):\n",
    "    fake_loss = adversarial_loss(state[DISC_GEN_DET], torch.zeros(state[DISC_REAL].shape[0], 32, dataset.example_length-2, device=DEVICE))\n",
    "    real_loss = adversarial_loss(state[DISC_REAL], torch.zeros(state[DISC_REAL].shape[0], 32, dataset.example_length-2, device=DEVICE))\n",
    "    state[G_LOSS] = adversarial_loss(state[DISC_GEN], torch.ones(state[DISC_REAL].shape[0], 32, dataset.example_length-2, device=DEVICE))\n",
    "    state[D_LOSS] = (real_loss + fake_loss) / 2\n",
    "    return state[G_LOSS] + state[D_LOSS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torchbearer.metrics.running_mean\n",
    "@torchbearer.metrics.mean\n",
    "class g_loss(torchbearer.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__('g_loss')\n",
    "\n",
    "    def process(self, state):\n",
    "        return state[G_LOSS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torchbearer.metrics.running_mean\n",
    "@torchbearer.metrics.mean\n",
    "class d_loss(torchbearer.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__('d_loss')\n",
    "\n",
    "    def process(self, state):\n",
    "        return state[D_LOSS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN(dataset.example_length-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.encoding_length = dataset.example_length-2\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "torchbearertrial = torchbearer.Trial(model, optim, criterion=None, metrics=['loss', g_loss(), d_loss()],\n",
    "                                     callbacks=[loss_callback, torchbearer.callbacks.live_loss_plot.LiveLossPlot(on_batch=True, max_cols=3, on_epoch=False)],\n",
    "                                     verbose=0, pass_state=True).to(DEVICE)\n",
    "torchbearertrial.with_train_generator(traingen)\n",
    "torchbearertrial.run(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z = torch.Tensor(np.random.normal(0, 1, (1, 32, 13))).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    stft_hat = model.generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_hat_np = stft_hat.cpu().squeeze().numpy()\n",
    "stft_hat_np = stft_hat_np[:513, :] + 1j * stft_hat_np[513:, :]\n",
    "audio_hat_np = librosa.istft(stft_hat_np, hop_length=512, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (24, 4)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1)\n",
    "\n",
    "ax1.plot(audio_hat_np[512:-512])\n",
    "ax1.set_title('Autoencoded frame')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_hat_np[128:-128], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
